@article{Tanaka2006,
abstract = {Tuberculosis can be studied at the population level by genotyping strains of Mycobacterium tuberculosis isolated from patients. We use an approximate Bayesian computational method in combination with a stochastic model of tuberculosis transmission and mutation of a molecular marker to estimate the net transmission rate, the doubling time, and the reproductive value of the pathogen. This method is applied to a published data set from San Francisco of tuberculosis genotypes based on the marker IS6110. The mutation rate of this marker has previously been studied, and we use those estimates to form a prior distribution of mutation rates in the inference procedure. The posterior point estimates of the key parameters of interest for these data are as follows: net transmission rate, 0.69/year [95{\%} credibility interval (C.I.) 0.38, 1.08]; doubling time, 1.08 years (95{\%} C.I. 0.64, 1.82); and reproductive value 3.4 (95{\%} C.I. 1.4, 79.7). These figures suggest a rapidly spreading epidemic, consistent with observations of the resurgence of tuberculosis in the United States in the 1980s and 1990s. Copyright {\textcopyright} 2006 by the Genetics Society of America.},
author = {Tanaka, Mark M. and Francis, Andrew R. and Luciani, Fabio and Sisson, S. A.},
doi = {10.1534/genetics.106.055574},
issn = {00166731},
journal = {Genetics},
mendeley-groups = {Edinburgh{\_}thesis/bibliography},
pmid = {16624908},
title = {{Using approximate bayesian computation to estimate tuberculosis transmission parameters from genotype data}},
year = {2006}
}

@inproceedings{Chen2019,
abstract = {Approximate Bayesian computation (ABC) is a set of techniques for Bayesian inference when the likelihood is intractable but sampling from the model is possible. This work presents a simple yet effective ABC algorithm based on the combination of two classical ABC approaches â€” regression ABC and sequential ABC. The key idea is that rather than learning the posterior directly, we first target another auxiliary distribution that can be learned accurately by existing methods, through which we then subsequently learn the desired posterior with the help of a Gaussian copula. During this process, the complexity of the model changes adaptively according to the data at hand. Experiments on a synthetic dataset as well as three real-world inference tasks demonstrates that the proposed method is fast, accurate, and easy to use.},
author = {Chen, Yanzhi and Gutmann, Michael U},
booktitle = {Proceedings of Machine Learning Research},
file = {:home/ntipakos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Gutmann - 2019 - Adaptive Gaussian Copula ABC.pdf:pdf},
mendeley-groups = {Edinburgh{\_}thesis/Theory,Edinburgh{\_}thesis,Edinburgh{\_}thesis/bibliography},
pages = {1584--1592},
title = {{Adaptive Gaussian Copula ABC}},
url = {http://proceedings.mlr.press/v89/chen19d.html},
volume = {89},
year = {2019}
}

@misc{Gutmann2016,
abstract = {Our paper deals with inferring simulator-based statistical models given some observed data. A simulator-based model is a parametrized mechanism which specifies how data are generated. It is thus also referred to as generative model. We assume that only a finite number of parameters are of interest and allow the generative process to be very general; it may be a noisy nonlinear dynamical system with an unrestricted number of hidden variables. This weak assumption is useful for devising realistic models but it renders statistical inference very difficult. The main challenge is the intractability of the likelihood function. Several likelihood-free inference methods have been proposed which share the basic idea of identifying the parameters by finding values for which the discrepancy between simulated and observed data is small. A major obstacle to using these methods is their computational cost. The cost is largely due to the need to repeatedly simulate data sets and the lack of knowledge about how the parameters affect the discrepancy. We propose a strategy which combines probabilistic modeling of the discrepancy with optimization to facilitate likelihood-free inference. The strategy is implemented using Bayesian optimization and is shown to accelerate the inference through a reduction in the number of required simulations by several orders of magnitude.},
archivePrefix = {arXiv},
arxivId = {1501.03291},
author = {Gutmann, Michael U. and Corander, Jukka},
booktitle = {Journal of Machine Learning Research},
eprint = {1501.03291},
file = {:home/ntipakos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gutmann, Corander - 2016 - Bayesian optimization for likelihood-free inference of simulator-based statistical models.pdf:pdf},
issn = {15337928},
keywords = {Approximate Bayesian computation,Bayesian inference,Computational efficiency,Intractable likelihood,Latent variables},
mendeley-groups = {Edinburgh{\_}thesis/Theory,Edinburgh{\_}thesis/bibliography},
title = {{Bayesian optimization for likelihood-free inference of simulator-based statistical models}},
year = {2016}
}
@inproceedings{Meeds2015,
abstract = {We describe an embarrassingly parallel, anytime Monte Carlo method for likelihood-free models. The algorithm starts with the view that the stochasticity of the pseudo-samples generated by the simulator can be controlled externally by a vector of random numbers u, in such a way that the outcome, knowing u, is deterministic. For each instantiation of u we run an optimization procedure to minimize the distance between summary statistics of the simulator and the data. After reweighing these samples using the prior and the Jacobian (accounting for the change of volume in transforming from the space of summary statistics to the space of parameters) we show that this weighted ensemble represents a Monte Carlo estimate of the posterior distribution. The procedure can be run embarrassingly parallel (each node handling one sample) and anytime (by allocating resources to the worst performing sample). The procedure is validated on six experiments.},
archivePrefix = {arXiv},
arxivId = {1506.03693},
author = {Meeds, Edward and Welling, Max},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1506.03693},
file = {:home/ntipakos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Meeds, Welling - 2015 - Optimization Monte Carlo Efficient and embarrassingly parallel likelihood-free inference.pdf:pdf},
issn = {10495258},
mendeley-groups = {Edinburgh{\_}thesis/Theory,Edinburgh{\_}thesis/bibliography},
title = {{Optimization Monte Carlo: Efficient and embarrassingly parallel likelihood-free inference}},
year = {2015}
}

@article{Ikonomov2019,
abstract = {This paper is on Bayesian inference for parametric statistical models that are implicitly defined by a stochastic simulator which specifies how data is generated. While exact sampling is possible, evaluating the likelihood function is typically prohibitively expensive. Approximate Bayesian Computation (ABC) is a framework to perform approximate inference in such situations. While basic ABC algorithms are widely applicable, they are notoriously slow and much research has focused on increasing their efficiency. Optimisation Monte Carlo (OMC) has recently been proposed as an efficient and embarrassingly parallel method that leverages optimisation to accelerate the inference. In this paper, we demonstrate a previously unrecognised important failure mode of OMC: It generates strongly overconfident approximations by collapsing regions of similar or near-constant posterior density into a single point. We propose an efficient, robust generalisation of OMC that corrects this. It makes fewer assumptions, retains the main benefits of OMC, and can be performed either as part of OMC or entirely as post-processing. We demonstrate the effectiveness of the proposed Robust OMC on toy examples and tasks in inverse-graphics where we perform Bayesian inference with a complex image renderer.},
archivePrefix = {arXiv},
arxivId = {1904.00670},
author = {Ikonomov, Borislav and Gutmann, Michael U.},
eprint = {1904.00670},
file = {:home/ntipakos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ikonomov, Gutmann - 2019 - Robust Optimisation Monte Carlo.pdf:pdf},
mendeley-groups = {Edinburgh{\_}thesis/Theory,Edinburgh{\_}thesis,Edinburgh{\_}thesis/bibliography},
title = {{Robust Optimisation Monte Carlo}},
url = {http://arxiv.org/abs/1904.00670},
year = {2019}
}
@article{Lintusaari2017,
abstract = {Bayesian inference plays an important role in phylogenetics, evolutionary biology, and in many other branches of science. It provides a principled framework for dealing with uncertainty and quantifying how it changes in the light of new evidence. For many complex models and inference problems, however, only approximate quantitative answers are obtainable. Approximate Bayesian computation (ABC) refers to a family of algorithms for approximate inference that makes a minimal set of assumptions by only requiring that sampling from a model is possible.We explain here the fundamentals of ABC, review the classical algorithms, and highlight recent developments.},
author = {Lintusaari, Jarno and Gutmann, Michael U. and Dutta, Ritabrata and Kaski, Samuel and Corander, Jukka},
doi = {10.1093/sysbio/syw077},
file = {:home/ntipakos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lintusaari et al. - 2017 - Fundamentals and recent developments in approximate Bayesian computation.pdf:pdf},
issn = {1076836X},
journal = {Systematic Biology},
keywords = {ABC,Approximate Bayesian computation,Bayesian inference,Likelihood-free inference,Phylogenetics,Simulator-based models,Stochastic simulation models,Treebased models},
mendeley-groups = {Edinburgh{\_}thesis/Theory,Edinburgh{\_}thesis,Edinburgh{\_}thesis/bibliography},
number = {1},
pages = {e66--e82},
title = {{Fundamentals and recent developments in approximate Bayesian computation}},
volume = {66},
year = {2017}
}


@misc{1708.00707,
Author = {Jarno Lintusaari and Henri Vuollekoski and Antti KangasrÃ¤Ã¤siÃ¶ and Kusti SkytÃ©n and Marko JÃ¤rvenpÃ¤Ã¤ and Pekka Marttinen and Michael Gutmann and Aki Vehtari and Jukka Corander and Samuel Kaski},
Title = {ELFI: Engine for Likelihood Free Inference},
Year = {2018},
Eprint = {arXiv:1708.00707},
}