\subsection{Algorithmic Description of ROMC}

\subsubsection{ROMC as a Meta-Algorithm}

As stated in the Introduction, ROMC can be understood as step-by-step alorithmic approach for perfroming the inference in Simulator-Based Models. The particular methods used for solving the sub-tasks are left as a free choice to the user. As presented in Algorithm~\ref{alg:meta_alg}, the methods involved in solving the optimistation problem (step~\ref{algstep:optimise}) and constructing the bounding box (step~\ref{algstep:bounding_box}) are not restricted. The practiosioner may choose any convenient algorithm, judging the trade-offs between accuracy, robustness, efficiency and complexity. In particular for the optimisation step, the choice of the appropriate optimiser should also consider the nature of the simulator that defines the properties of $g_i(\thetab)$; for example, is the function differentiable, do we know the gradients $\nabla_{\thetab} [g_i] $ in closed-form or they should be approximated by finite differences. As described in sections~\ref{subsubsec:GB_approach} and~\ref{subsubsec:GP_approach}, ROMC proposes two alternative optimisation schemes (gradient-based and gaussian-process approach) depending on whether the gradients are available or not.

\begin{algorithm}[t]
	\caption{ROMC as a Meta-Algorithm. Requires $M_r(\theta), y_0$. Hyperparameters $n_1,n_2$.}\label{alg:meta_alg}
	\begin{algorithmic}[1]
		\For{$i \gets 1 \textrm{ to } n_1$}
    \State Sample a random state $\vb_i \sim p(\vb)$
		\State Define the deterministic mapping $f_i(\thetab) = M_d(\thetab, \vb)$ and therefore $g_i(\thetab) = d(f_i(\theta), y_0)$.
    \State Obtain $d_i^* = \text{min}_{\thetab} \: [g_i(\thetab)]$ and $\thetab_i^* = \text{argmin}_{\thetab}\: [g_i(\thetab)]$ using any convenient optimiser. \label{algstep:optimise}
    \State Approximate the local area $\{ \thetab : g_i(\thetab) < \epsilon$ and $d(\thetab, \thetab_i^*) < M \}$ with a Bounding Box, using any convenient method. \label{algstep:bounding_box}
		\State Define a uniform distribution $q_i(\thetab)$ over the Bounding Box.
			\For{$j \gets 1 \textrm{ to } n_2$}
			\State $\thetabij \sim q_i(\thetab)$
			\State Accept $\thetabij$ as posterior sample with weight $w_{ij} = \frac{p(\thetabij)}{q_i(\thetabij)} \indicator{\regioni} (\thetabij)$
			\EndFor
      \EndFor
     \Return(List with samples $\thetabij$ and weights $w_{ij}$) 
	\end{algorithmic}
\end{algorithm}


\subsubsection{Training and Inference Algorithms}
\label{subsubsec:algorithmic_description}

In this section, we will provide the algorithmic description of the ROMC method; (a) the solution of the optimisation problems using either the gradient based approach or the Gaussian Process alternative and (b) the construction of the Bounding Box. Afterwards, we will discuss the advantages and the disadvantages of each choice both in terms of accuracy and efficiency.

\noindent
At a high-level, the ROMC method can be split into the training and the inference part.

At the training (fitting) part, the method samples the nuisance variables $v_i \sim p(v)$, defines the the optimisation problems $\min_\theta [g_i(\theta)]$, solves them to obtain $\theta_i^*$, checks whether the optimal point the respects the constraint and finally builds the bounding box for obtaining the proposal region $q_i$. Using the Gaussian Process set up makes the training part slower due to the fitting of the surrogate model at step 2. On the other hand, computing the $q_i$ becomes faster since the evaluating the distance doesn't involve running the whole simulator $M_d^i(\theta)$ for each query point. The algorithms are presented in~\ref{alg:training_GB} and~\ref{alg:training_GP}.

Performing the infernce part includes evaluating the unnormalised posterior and sampling from the posterior. Computing an expectation is not described as a distinct phase, since it can be done in straightforward manner using the weighted samples. For evaluating the unnormalized posterior in the Gradient-Based approach, only the deterministic functions $g_i$ and the prior distribution $p(\theta)$ are required; there is no need for solving the optimisation problems and building the proposal regions. The evaluation requires iterating over all $g_i$ and evaluating the distance from the observed data. In contrast, using the GP approach, there is need for the optimisation part to run for fitting the surrogate models $\hat{d}_i(\theta)$. Afterwards the distance is evaluated on them. The evaluation of the posterior is presented analytically in~\ref{alg:posterior_GB} and~\ref{alg:posterior_GP}.

Weighted Sampling is performed by getting $n_2$ samples from each proposal region $q_i$, evaluating if the actually fall inside the acceptance region and if so, compute their weight. The procedure is identical in both cases, apart from step 3 where the acceptance check is done in the real model $g_i$ in the Gradient-Based approach and in the surrogate model $\hat{d}_i$ otherwise. The sampling algorithms are presented step-by-step in~\ref{alg:sampling_GB} and~\ref{alg:sampling_GP}.

\begin{minipage}{0.46\textwidth}
\begin{algorithm}[H]
    \centering
    \caption{Training Part - Gradient approach. Requires $g_i(\theta), p(\theta)$}\label{alg:training_GB}
    \begin{algorithmic}[1]
      \For{$i \gets 1 \textrm{ to } n$}
        \State Obtain $\theta_i^*$ using a Gradient Optimiser
        \If{$g_i(\theta_i^*) > \epsilon$}
        \State{go to} 1
        \Else
        \State Approximate $H_i \approx J^T_iJ_i$
        \State Use algorihm~\ref{alg:region_construction} to obtain $q_i$
        \EndIf      
      \EndFor
      \Return{$q_i, p(\theta), g_i(\theta)$}
    \end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}{0.46\textwidth}
\begin{algorithm}[H]
    \centering
    \caption{Training Part - GP approach. Requires $g_i(\theta), p(\theta)$}\label{alg:training_GP}
    \begin{algorithmic}[1]
      \For{$i \gets 1 \textrm{ to } n$}
        \State Obtain $\theta_i^*, \hat{d}_i(\theta)$ using a GP approach
        \If{$g_i(\theta_i^*) > \epsilon$}
        \State{go to} 1
        \Else
        \State Approximate $H_i \approx J^T_iJ_i$
        \State Use algorihm~\ref{alg:region_construction} to obtain $q_i$
        \EndIf      
      \EndFor
      \Return{$q_i, p(\theta), \hat{d}_i(\theta)$}
    \end{algorithmic}
\end{algorithm}
\end{minipage}

\begin{algorithm}[!ht]
	\caption{Proposal Region $q_i$ construction; Needs, a model of distance $d$ ($\hat{d}$ or $g_i$), optimal point $\theta_i^*$, number of refinements $K$, step size $\eta$ and curvature matrix $\hessian_i$ ($J_i^TJ_i $ or GP Hessian)}\label{alg:region_construction}
	\begin{algorithmic}[1]
	\State Compute eigenvectors $\mathbf{v}_{d}$ of $H_i$ {\scriptsize ($d = 1,\ldots,||\theta ||)$}
	\For{$d \gets 1 \textrm{ to } ||\theta||$}
		\State $\Tilde{\theta} \gets \theta_i^*$ \label{algstep:box_constr_start}
		\State $k \gets 0$
		\Repeat
        	\Repeat
                \State $\Tilde{\theta} \gets \Tilde{\theta} + \eta \ \mathbf{v}_{d}$ \Comment{Large step size $\eta$.}
        	\Until{$d( (\Tilde{\theta}, i), ) \ge \epsilon$}
        	\State $\Tilde{\theta} \gets \Tilde{\theta} - \eta \ \mathbf{v}_{d}$
        	\State $\eta \gets \eta/2$ \Comment{More accurate region boundary}
        	\State $k \gets k + 1$
    	\Until $k = K$
    	\State Set final $\Tilde{\theta}$ as region end point. \label{algstep:box_constr_end}
    	\State Repeat steps~\ref{algstep:box_constr_start}~-~\ref{algstep:box_constr_end} for $\mathbf{v}_{d} = - \mathbf{v}_{d}$
	\EndFor
	\State Fit a rectangular box around the region end points and define $q_i$ as uniform distribution
	\end{algorithmic}
\end{algorithm}

\begin{minipage}{0.46\textwidth}
\begin{algorithm}[H]
    \centering
    \caption{Evaluate unnormalised posterior - Gradient approach. Requires $g_i(\theta), p(\theta)$}\label{alg:posterior_GB}
    \begin{algorithmic}[1]
      \State $k \leftarrow 0$
        \For {$i \gets 1 \textrm{ to } n_1$}
          \If {$g_i(\theta) > \epsilon$}
            \State $k \leftarrow k + 1$
          \EndIf
          \EndFor
      \Return{$kp(\theta)$}
    \end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}{0.46\textwidth}
\begin{algorithm}[H]
    \centering
    \caption{Evaluate unnormalised posterior - GP approach. Requires $\hat{d}_i(\theta), p(\theta)$}\label{alg:posterior_GP}
    \begin{algorithmic}[1]
      \State $k \leftarrow 0$
        \For {$i \gets 1 \textrm{ to } n_1$}
          \If {$d_i(\theta) > \epsilon$}
            \State $k \leftarrow k + 1$
          \EndIf
          \EndFor
      \Return{$kp(\theta)$}
    \end{algorithmic}
\end{algorithm}
\end{minipage}


\begin{minipage}{0.46\textwidth}
\begin{algorithm}[H]
    \centering
    \caption{Sampling - Gradient Based approach. Requires $g_i(\theta), p(\theta), q_i$}\label{alg:sampling_GB}
    \begin{algorithmic}[1]
      \For {$i \gets 1 \textrm{ to } n_1$}
      \For {$j \gets 1 \textrm{ to } n_2$}
          \State $\theta_{ij} \sim q_i$
          \If {$g_i(\theta_{ij}) > \epsilon$}
            \State Reject $\theta_{ij}$
          \Else {}
            \State $w_{ij} = \frac{p(\theta_{ij})}{q(\theta_{ij})}$
            \State Accept $\theta_{ij}$, with weight $w_{ij}$
          \EndIf
      \EndFor
      \EndFor
    \end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}{0.46\textwidth}
\begin{algorithm}[H]
    \centering
    \caption{Sampling - GP approach. Requires $\hat{d}_i(\theta), p(\theta), q_i$}\label{alg:sampling_GP}
    \begin{algorithmic}[1]
      \For {$i \gets 1 \textrm{ to } n_1$}
      \For {$j \gets 1 \textrm{ to } n_2$}
          \State $\theta_{ij} \sim q_i$
          \If {$\hat{d}_i(\theta_{ij}) > \epsilon$}
            \State Reject $\theta_{ij}$
          \Else {}
            \State $w_{ij} = \frac{p(\theta_{ij})}{q(\theta_{ij})}$
            \State Accept $\theta_{ij}$, with weight $w_{ij}$
          \EndIf
      \EndFor
      \EndFor
    \end{algorithmic}
\end{algorithm}
\end{minipage}

\subsection{Computational Complexity}

In the notation, we use $S$ for describing the size of the Simulator. In table we observe that constructing the regions and sampling are the most demanding operations, though $n_2 >> K$. In the Gaussian-Process set-up sampling is not influenced by the size of the simulator.

\begin{tabular}{|c||c|c|}
 \hline
 \multicolumn{3}{|c|}{Compuational Complexity} \\
  \hline
  & Gradient-Based     & Gaussian Process\\
  \hline
  \multirow{2}{3.5em}{Optimize}  & Closed-Form Gradients: $\bigO(Sn_1)$   &  \\
                               & Approximate Gradients: $\bigO(DSn_1)$  &  \\  
  \hline
  Construct Regions    & $\bigO(KDSn_1)$  & $\bigO(KDn_1)$ \\
  \hline
  Evaluate Posterior   & $\bigO(n_1S)$  & $\bigO(n_1)$ \\
  \hline
  Sampling             & $\bigO(n_1n_2DS)$ & $\bigO(n_1n_2D)$ \\
 \hline
\end{tabular}
