\subsection{Simulator-Based (Implicit) Models}

As already stated, in Simulator-Based models we cannot evaluate the
posterior $p(\thetab|\data) \propto L(\thetab)p(\thetab)$, since it is
impossible to evaluate the likelihood $L(\thetab) =
p(\data|\thetab)$. So, we have to use the following property in order
to take advantage of the simulator $M_r(\thetab)$.

\begin{equation} \label{eq:likelihood} L(\thetab) = \lim_{\epsilon \to
    0} c_\epsilon \int_{\yb \in B_\epsilon(\data)} p(\yb|\thetab)d\yb
  = \lim_{\epsilon \to 0} c_\epsilon Pr(M_r(\thetab) \in
  B_\epsilon(\data))
\end{equation}

\subsubsection{Approximate Bayesian Computation (ABC) Rejection
  Sampling}

ABC Rejection Sampling is a modified version of Rejection Sampling,
for cases when likelihood evaluation is intractable. In typical
Rejection Sampling, a sample is obtained from the prior
$\thetab \sim p(\thetab)$ and it is maintained with probability
$L(\thetab)/ \text{max}_{\thetab} L(\thetab)$. Although we cannot use
this approach out of the box (evaluating $L(\thetab)$ is impossible in
our case), we can take advantage of the simulator.

In the discrete case scenario, where $\Y_{\thetab}$ can take a finite
set of numbers, the likelihood becomes
$L(\thetab) = Pr(\Y_{\thetab} = \data)$ and the posterior
$p(\thetab|\data) \propto Pr(\Y_{\thetab}=\data)p(\thetab)$. We can
sample from the prior $\thetab_i \sim p(\thetab)$, run the simulator
$\yb_i = M_r(\thetab_i)$ and maintain $\thetab_i$ only if
$\yb_i = \data$.

The method above becomes less usefull as the finite set of
$\Y_{\thetab}$ values grows larger, since the probability of
maintaining a sample becomes smaller. In the limit where the set
becomes infinite (i.e. continuous case) the probability becomes
zero. In order for the method to work in this set-up, a relaxation is
introduced; we relax the acceptance criterion by letting
$\Y_{\thetab}$ lie in a larger set of points i.e.
$\Y_{\theta} \in \region(\data), \epsilon > 0$. The region can be
defined as $\region (\data) := \{\yb: d(\yb, \data) < \epsilon \}$
where $d(\cdot, \cdot)$ can represent any valid distance. With this
modification, the maintained samples follow the approximate posterior,

\begin{equation} \label{eq:approx_posterior}
  p_{d,\epsilon}(\thetab|\data) \propto Pr(\Y_{\thetab} \in
  \region(\data) p(\thetab)
\end{equation}

\noindent
This method is called \textit{Rejection ABC} and forms the basis of
Likelihood-Free methods.

\subsubsection{Summary Statistics}

When the dimensionality of $\Y_{\thetab} \in \mathbb{R}^D$ is high,
generating samples inside $\region (\data)$ becomes rare even with
large $\epsilon$; this is the curse of dimensionality. As a
representative example if (a) $d$ is set to be the euclidean distance
the $\region(\data) := \{ \yb: ||\yb - \data||_2^2 < \epsilon^2 \}$ is
a hyper-sphere with radius $\epsilon$ and (b) the prior distribution
$p(\thetab)$ is a uniform distribution in a hyper-cube with side of
length $2\epsilon$, then the probability of drawing a sample inside
the hyper-sphere becomes:

\begin{equation}
  Pr(\Y_{\thetab} \in \region (\data)) = Pr(\thetab \in \region (\data)) = \frac{V_{hypersphere}}{V_{hypercube}} = \frac{\pi^{D/2}}{D2^{D-1}\Gamma(D/2)} \rightarrow 0, \quad \text{as} \quad D \rightarrow \infty
\end{equation}

\noindent 
We observe that the probality tends to $0$, independently of
$\epsilon$; enlarging $\epsilon$ will not increase the acceptance
rate. This produces the need for a mapping
$T: \mathbb{R}^{D_1} \rightarrow \mathbb{R}^{D_2}$ where $D_1 > D_2$,
redefining the area as
$\region(\data) := \{\yb: d(T(\yb), T(\data)) < \epsilon \}$. This
dimensioanlity-reduction is called \textit{summary statistic}.

\subsubsection{Approximations Introduced}

Approximating the posterior as
$p_{d,\epsilon}(\thetab|\data) \propto Pr(\Y_{\thetab} \in
\region(\data))p(\thetab)$ where
$\region(\data) := \{\yb: d(T(\yb), T(\data)) < \epsilon \}$
introduces two different types approximation error:

\begin{itemize}
\item $\epsilon$ is chosen to be large enough, so that enough samples
  are accepted
\item $T$ introduces loss of information, making possible a $\yb$ far
  away from the $\data$, namely $\yb: d(\yb,data)>\epsilon$, to enter
  the acceptance region at after the dimensionality reduction
  $d(T(\yb), T(\data) < \epsilon)$
\end{itemize}

\noindent
In the following sections we will not use the summary statistics in
our expressions, for the notation not to clutter. Though, all the
following propositions are valid with the use of summary statistics.
  
\subsubsection{Optimization Monte Carlo (OMC)}

Based on $\region$, we can define two useful entities; an indicator
function and a conditional distribution.

\subsubsection*{Indicator Function}

The indicator function $\indicator{\region(\yb)}(\xb)$ returns 1 if
$\xb \in \region(\yb)$ and 0 otherwise. If $d(\cdot,\cdot)$ is a
formal distance, due to symmetry
$\indicator{\region(\yb)}(\xb) = \indicator{\region(\xb)}(\yb)$.

\begin{gather} \label{eq:indicator} \indicator{\region(\yb)}(\xb)=
  \left\{
    \begin{array}{ll}
      1 & \mbox{if } \xb \in \region(\yb) \\
      0 & \mbox{else } 
    \end{array} \right. \end{gather}

\subsubsection*{Boxcar Kernel}

The boxcar kernel is the conditional distribution:

\begin{gather}
  p_{d,\epsilon}(\yb|\xb) = \left\{
    \begin{array}{ll}
      c  & \mbox{if } d(\yb,\xb) \leq \epsilon \\
      0 & \mbox{else } 
    \end{array}
  \right. \text{where} c = \frac{1}{\int_{ \{ \yb: d(\yb,\xb) <
      \epsilon \} } d\yb}
\end{gather}
%
If we understand the boxcar kernel as a data generation process we can
make two important notices:

\begin{itemize}
\item given a specific $\xb$, all values
  $\yb: \yb \in B_{d,\epsilon}(\xb)$ have equal probability to be
  generated
\item if a specific $\yb$ value has been generated, all
  $\xb: x \in B_{d,\epsilon}(\yb)$ have equal probability to be the
  conditional value that lead to this generation
\end{itemize}
%
Finally, we can also observe that the kernel can be defined through
the indicator function:

\begin{equation}
  p_{d,\epsilon}(\yb|\xb) = c \mathbb{1}_{B_{d,\epsilon}(\yb)}(\xb) = c \mathbb{1}_{B_{d,\epsilon}(\xb)}(\yb)
\end{equation}

\subsubsection*{Initial View}

Based on~\ref{eq:approx_posterior}, we can approximate the likelihood as:

\begin{gather} \label{eq:primal_view}
  L_{d, \epsilon}(\thetab)=\int_{B_\epsilon(\data)}p(\yb|\thetab)dy = \int p_{d,\epsilon}(\data|\yb)p(\yb|\thetab)d\yb\\
  \approx \frac{1}{N} \sum_i^N p_{d,\epsilon} (\data|\yb_i) \\
  \approx \frac{c}{N} \sum_i^N \indicator{\region(\yb_i)}(\data), \yb_i \sim M_r(\thetab)
\end{gather}
%
This approach is quite intuitive; approximating the likelihood of a
specific $\thetab$ requires sampling from the data generator and count
the fraction of samples that lie close to the observations.
Nevertheless, for every distinct evaluation of
$L_{d,\epsilon}(\thetab)$ $N$ new samples are needed. This make this
approach quite inconvenient.

\subsubsection*{Alternative View}

For overcoming the disadvantage introduced above, OMC attempts an
alternative approximation. It samples all the nuisance variables from
a distribution $v_i \sim p(v)$ and it converts the random simulator to
a deterministic mapping $M_d(\theta, v_i)$,

\begin{gather} 
  L_{d,\epsilon}(\theta)=\int_{B_\epsilon(y_0)}p(y|\theta)dy = \int p_{d,\epsilon}(y_0|y)p(y|\theta)dy\\
  = \int_y \int_v p_{d,\epsilon}(y_0|y)p(y|\theta, v) p(v)dxdv \\
  = \int_v p_{d,\epsilon}(y_0|y=M_d(\theta, v)) p(v)dv \\
  \approx \frac{1}{N} \sum_i^N p_{d,\epsilon} (y_0|y=M_d(\theta, v_i)) \\
  \approx \frac{c}{N} \sum_i^N \mathbb{1}_{B_{d,\epsilon}(M_d(\theta,
    v_i))} (y_0), v_i \sim p(v)
  \label{eq:alt_view}
\end{gather}
%
Based on this approach, the unnormalized approximate posterior can be
defined as:

\begin{equation} \label{eq:posterior} p_{d,\epsilon}(\theta|y_0)
  \propto p(\theta) \sum_i^N \mathbb{1}_{B_{d,\epsilon}(M_d(\theta,
    v_i))} (y_0)
\end{equation}
%
Forming an analogy with the previous approach, we sample many nuisance
variables in order to absorb the randomness of the generator and we
count the fraction of times the deterministic generator produces mapps
to outputs close to the observed data. Though it is conceptually close
to the previous approach, this approach has a major advantage; we can
sample the nuisance variables once (training part) and afterwards
evaluate every $\theta$ based on a predefined expression (inference
part).
