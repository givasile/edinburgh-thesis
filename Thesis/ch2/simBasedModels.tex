As already stated at Chapter~\ref{sec:introduction}, in
simulator-based models we cannot evaluate the posterior
$p(\thetab|\data) \propto L(\thetab)p(\thetab)$, due to the
intractability of the likelihood $L(\thetab) = p(\data|\thetab)$. The
following equation allows incorporating the simulator in the place of
the likelihood and forms the basis of all likelihood-free inference
approaches,

\begin{equation} \label{eq:likelihood}
  L(\thetab) =
  \lim_{\epsilon \to 0} c_\epsilon \int_{\yb \in B_{d,\epsilon}(\data)} p(\yb|\thetab)d\yb =
  \lim_{\epsilon \to 0} c_\epsilon \Pr(M_r(\thetab) \in B_{d,\epsilon}(\data))
\end{equation}
%
where $c_\epsilon$ is a proportionality factor dependent on
$\epsilon$, needed when
$\Pr(M_r(\thetab) \in B_{d, \epsilon}(\data)) \rightarrow 0$, as
$\epsilon \rightarrow 0$. Equation~\ref{eq:likelihood} describes that
the likelihood of a specific parameter configuration $\thetab$ is
proportional to the probability that the simulator will produce
outputs equal to the observations, using this configuration.

\subsubsection{Approximate Bayesian Computation (ABC) Rejection
  Sampling}

ABC rejection sampling is a modified version of the traditional
rejection sampling method, for cases when the evaluation of the
likelihood is intractable. In the typical rejection sampling, a sample
obtained from the prior $\thetab \sim p(\thetab)$ gets accepted
with probability $L(\thetab)/ \text{max}_{\thetab}
L(\thetab)$. Though we cannot use this approach out-of-the-box
(evaluating $L(\thetab)$ is impossible in our case), we can
modify the method incorporating the simulator.

In the discrete case scenario where $\Y_{\thetab}$ can take a finite
set of values, the likelihood becomes
$L(\thetab) = \Pr(\Y_{\thetab} = \data)$ and the posterior
$p(\thetab|\data) \propto \Pr(\Y_{\thetab}=\data)p(\thetab)$; hence, we can
sample from the prior $\thetab_i \sim p(\thetab)$, run the simulator
$\yb_i = M_r(\thetab_i)$ and accept $\thetab_i$ only if
$\yb_i = \data$.

The method above becomes less useful as the finite set of
$\Y_{\thetab}$ values grows larger, since the probability of
accepting a sample becomes smaller. In the limit
where the set becomes infinite (i.e.\ continuous case) the probability
becomes zero. In order for the method to work in this set-up, a
relaxation is introduced; we relax the acceptance criterion by letting
$\yb_{i}$ lie in a larger set of points i.e.\
$\yb_{i} \in \region(\data), \epsilon > 0$. The region can be
defined as $\region (\data) := \{\yb: d(\yb, \data) \leq \epsilon \}$
where $d(\cdot, \cdot)$ can represent any valid distance. With this
modification, the maintained samples follow the approximate posterior,

\begin{equation} \label{eq:approx_posterior}
  p_{d,\epsilon}(\thetab|\data) \propto Pr(\yb \in
  \region(\data)) p(\thetab)
\end{equation}

\noindent
This method is called Rejection ABC.

\subsubsection{Summary Statistics}

When $\yb \in \mathbb{R}^D$ lies in a high-dimensional space, generating
samples inside $\region (\data)$ becomes rare even when $\epsilon$ is
relatively large; this is the curse of dimensionality. As a
representative example lets make the following hypothesis;

\begin{itemize}
\item $d$ is set to be the Euclidean distance, hence
  $\region(\data) := \{ \yb: ||\yb - \data||_2^2 < \epsilon^2 \}$ is a
  hyper-sphere with radius $\epsilon$ and volume $V_{hypersphere} = \frac{\pi^{D/2}}{\Gamma(D/2 + 1)} \epsilon^D$
\item the prior $p(\thetab)$ is a uniform distribution in a hyper-cube with side of
  length $2\epsilon$ and volume $V_{hypercube} = (2\epsilon)^D$
\item the generative model is the identity function $\yb=f(\thetab)= \thetab $
\end{itemize}

\noindent
The probability of drawing a sample inside the hypersphere equals the
fraction of the volume of the hypersphere inscribed in the hypercube:

\begin{equation}
  Pr(\yb \in \region (\data))
  = Pr(\thetab \in \region (\data))
  = \frac{V_{hypersphere}}{V_{hypercube}}
  = \frac{\pi^{D/2}}{2^D\Gamma(D/2 + 1)} \rightarrow 0, \quad \text{as} \quad D \rightarrow \infty
\end{equation}

\noindent
We observe that the probability tends to $0$, independently of
$\epsilon$; enlarging $\epsilon$ will not increase the acceptance
rate. Intuitively, we can think that in high-dimensional spaces the
volume of the hypercube concentrates at its corners. This generates
the need for a mapping
$T: \mathbb{R}^{D_1} \rightarrow \mathbb{R}^{D_2}$ where $D_1 > D_2$,
for squeezing the dimensionality of the output. This
dimensionality-reduction step that redefines the area as
$\region(\data) := \{\yb: d(T(\yb), T(\data)) \leq \epsilon \}$ is
called \textit{summary statistic} extraction, since the distance is
not measured on the actual outputs, but on a summarisation (i.e.\
lower-dimension representation) of them.

\subsubsection{Approximations introduced so far}

So far, we have introduced some approximations for inferring the
posterior as
$p_{d,\epsilon}(\thetab|\data) \propto Pr(\Y_{\thetab} \in
\region(\data))p(\thetab)$ where
$\region(\data) := \{\yb: d(T(\yb), T(\data)) < \epsilon \}$. These
approximations introduce two different types of errors:

\begin{itemize}
\item $\epsilon$ is chosen to be \textit{big enough}, so that enough
  samples are accepted. This modification leads to the approximate
  posterior introduced in \eqref{eq:approx_posterior}
\item $T$ introduces some loss of information, making possible a $\yb$
  far away from $\data$ i.e.\ $\yb: d(\yb,\data)>\epsilon$, to enter
  the acceptance region after the dimensionality reduction
  $d(T(\yb), T(\data)) \leq \epsilon$
\end{itemize}

\noindent
In the following sections, we will not use the summary statistics in
our expressions for the notation not to clutter. One could understand
it as absorbing the mapping $T(\cdot)$ inside the simulator. In any
case, all the propositions that will be expressed in the following
sections are valid with the use of summary statistics.
  
\subsubsection{Optimisation Monte Carlo (OMC)}

Before we define the likelihood approximation as introduced in the
OMC, approach lets define the indicator function based on
$\region(\yb)$. The indicator function $\indicator{\region(\yb)}(\xb)$
returns 1 if $\xb \in \region(\yb)$ and 0 otherwise. If
$d(\cdot,\cdot)$ is a formal distance, due to symmetry
$\indicator{\region(\yb)}(\xb) = \indicator{\region(\xb)}(\yb)$, so
the expressions can be used interchangeably.

\begin{gather} \label{eq:indicator} \indicator{\region(\yb)}(\xb)=
  \left\{
    \begin{array}{ll}
      1 & \mbox{if } \xb \in \region(\yb) \\
      0 & \mbox{otherwise} 
    \end{array} \right. \end{gather}

\noindent
Based on equation~\eqref{eq:approx_posterior} and the indicator
function as defined above~\eqref{eq:indicator}, we can approximate the
likelihood as:

\begin{gather} \label{eq:approx_likelihood}
  L_{d, \epsilon}(\thetab) =
  \int_{\yb \in B_\epsilon(\data)}p(\yb|\thetab)d\yb =
  \int_{\yb \in \R^D} \indicator{\region(\data)}(\yb)p(\yb|\thetab)d\yb\\
  \approx \frac{1}{N} \sum_i^N \indicator{\region(\data)}(\yb_i),\text{ where }
  \yb_i \sim M_r(\thetab) \label{eq:init_view}\\
  \approx \frac{1}{N} \sum_i^N \indicator{\region (\data)} (\yb_i)
  \text{ where } \yb_i = M_d(\thetab, \vb_i), \vb_i \sim p(\vb) \label{eq:alt_view}
\end{gather}
%
This approach is quite intuitive; approximating the likelihood of a
specific $\thetab$ requires sampling from the data generator and count
the fraction of samples that lie inside the area around the
observations. Nevertheless, by using the approximation of equation
\eqref{eq:init_view} we need to draw $N$ new samples for each distinct
evaluation of $L_{d,\epsilon}(\thetab)$; this makes this approach
quite inconvenient from a computational point-of-view. For this
reason, we choose to approximate the integral as in equation
\eqref{eq:alt_view}; the nuisance variables are sampled once
$\vb_i \sim p(\vb)$ and we count the fraction of samples that lie
inside the area using the deterministic simulators
$M_d(\thetab, \vb_i) \: \forall i$. Hence, the evaluation
$L_{d,\epsilon}(\thetab)$ for each different $\thetab$ does not imply
drawing new samples all over again. Based on this approach, the
unnormalised approximate posterior can be defined as:

\begin{equation} \label{eq:aprox_posterior}
  p_{d,\epsilon}(\thetab|\data)
  \propto p(\thetab) \sum_i^N \indicator{ \region(\data)} (\yb_i)
\end{equation}

\subsubsection*{Further approximations for sampling and computing expectations}

The posterior approximation in \eqref{eq:approx_posterior} does not
provide any obvious way for drawing samples. In fact, the set
$\mathcal{S}_i = \{ \thetab: M_d(\thetab, \vb_i) \in \region(\data) \}$ can
represent any arbitrary shape in the D-dimensional Euclidean space; it
can be non-convex, can contain disjoint sets of $\thetab$ etc. We need
some further simplification of the posterior for being able to draw
samples from it.

As a side-note, weighted sampling could be performed in a
straightforward fashion with importance sampling. Using the prior as
the proposal distribution $\thetab_i \sim p(\thetab)$ and we can
compute the weight as
$w_i = \frac{L_{d,\epsilon}(\thetab_i)}{p(\thetab_i)}$, where
$L_{d,\epsilon}(\thetab_i)$ is computed with the expression
\eqref{eq:approx_likelihood}. This approach has the same drawbacks as
ABC rejection sampling; when the prior is wide or the dimensionality
$D$ is high, drawing a sample with non-zero weight is rare, leading to
either poor Effective Sample Size (ESS) or huge execution time.

The OMC proposes a quite drastic simplification of the posterior; it
squeezes all regions $\mathcal{S}_i$ into a single point
$\thetab_i^* \in \mathcal{S}_i$ attaching a weight $w_i$ proportional
to the volume of $\mathcal{S}_i$. For obtaining a
$\thetab_i^* \in \mathcal{S}_i$, a gradient based optimiser is used
for minimising $g_i(\thetab) = d(\data, f_i(\thetab))$ and the
estimation of the volume of $\mathcal{S}_i$ is done using the Hessian
approximation $\hess_i \approx \jac_i^{*T}\jac_i^*$, where $\jac_i^*$
is the Jacobian matrix of $g_i(\thetab)$ at $\thetab_i^*$. Hence,

\begin{gather} \label{eq:OMC_posterior}
    p(\thetab|\data) \propto p(\thetab) \sum_i^N w_i \delta(\thetab - \thetab_i^*)\\
  \thetab_i^* = \text{argmin}_{\thetab} \:g_i(\thetab) \\
  w_i \propto \frac{1}{\sqrt{det( \jac_i^{*T}\jac_i^*)}}
\end{gather}

The distribution \eqref{eq:OMC_posterior} provides weighted samples
automatically and an expectation can be computed easily with the
following equation,

\begin{equation}
  \label{eq:OMC_expectation}
  E_{p(\thetab|\data)}[h(\thetab)] = \frac{\sum_i^N w_i p(\thetab_i^*)h(\thetab_i^*)}{\sum_i^N w_i p(\thetab_i^*)}
\end{equation}
