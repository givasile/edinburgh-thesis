\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}

\title{Summary of Optimization Monte Carlo methods}
\author{Vasileios Gkolemis}
\date{June 2020}

\newcommand{\norm}[1]{\lVert#1\rVert_2}

\begin{document}

\maketitle

\section{Symbols for functions/probabilities}

\begin{itemize}
    \item $p(x|y)$: any valid pdf function $g(x,y): \int_x g(x,y) = 1$. 
    \item $p(x|y=y_0)$ is a function of x (i.e. $g(x): \int_x g(x) = 1$)  
    \item $p(x=x_0|y)$ is a function of y (i.e. $g(y)$) 
    \item $p(x=x_0|y=y_0)$ is a real number $\kappa \in [0,1]$
    \item $p_\epsilon(x)$ is 
\end{itemize}

\section{Symbols for random variables or parameters}

\begin{itemize}
    \item $x, y$ are dummy variables, used as thresholders
    \item $D$ the available data (observed variables)
\end{itemize}



\section{Likelihood-Free methods}

In our models, the random variables are split into 2 categories; the observed ones (visibles) and the unobserved. The unobserved ones can be further split into 3 categories:

\begin{itemize}
    \item the parameters of the model $\theta$, over which we try to infer a posterior distribution
    \item the hidden/latent variables $h$, which we cannot observe but are part of the data generation process
    \item the nuisance variables $u$, which can be though as dummy random variables, that add stochasticity when sampling from a random variable. Simply speaking, those are the r.v. that, when known, can absorb the randomness of the sampling procedure $x \sim p(x|\theta)$, transforming it to a deterministic function $x = f(\theta, u)$
\end{itemize}

The particularity of likelihood-free methods is that the likelihood function $p(y=D|\theta)$ cannot be evaluated (i.e. cannot compute the number $p(y=D|\theta=\theta_0)$) but we are able to sample from the generative distribution $p(y|\theta)$. We can think that we hold a black box simulator (stochastic function) that maps the parameters to a sample $y = M_{rand}(\theta)$. The randomness of the mapping is caused by the unknown nuisance variables $u$ that are part of the generation procedure (e.g. calls to random generator etc). If we represent them with a vector, we can transform the aforementioned random procedure to a deterministic function $M(\theta, u) = y$.  

The ultimate goal is to estimate the posterior distribution over the parameters $\theta$ or at least to get some samples from it. Subsequently, we use the approximation to sample from the posterior (or the samples themselves) to estimate the predictive distribution. 

\begin{equation}
    p(\theta|y=D) = \frac{p(\theta) p(y=D|\theta)}{p(y=D)}
\end{equation}


\section{Background of Likelihood-Free methods and basic algorithms}

The fundamental idea of all Likelihood-Free methods is that we try to generate (artificial) data using our simulator $M_{rand}(\theta)$ that look-alike (are close) to the observed (real) data $D$. When they do, we hold the parameters $\theta$ that were part of the generation, as possible candidates for the posterior.

Since, in the contiguous case it is impossible to reproduce exactly the data, a relaxation is introduced:

\begin{equation} \label{eq:post_approx}
    p(\theta|y=D) \approx p(\theta|y \in B_\epsilon(D)) = \frac{p(\theta) p(y \in B_\epsilon(D)|\theta)}{p(y=D)}
\end{equation}

$B_\epsilon$ represents an area (normally sphere or bounding-box) around $D$ governed by the hyper-parameter $\epsilon$. In the limit where $\epsilon \rightarrow 0$, the approximation becomes equality.

When the data belongs to a high-dimensional space (high in our case can be even 10 dimensions), it is becomes difficult to reproduce the observed data $M_{rand}(\theta) \in B_{\epsilon}(D)$ without letting $\epsilon$ reach very big values. Hence, we normally use a way to squeeze our data to a lower dimensions; we use a function $T:\mathbf{R}^{D_1} \rightarrow \mathbf{R}^{D_2}$ where $D_1 > D_2$. Then, equation \ref{post_approx} becomes:

\begin{equation} \label{eq:summary_stat}
    p(\theta|y=D) \approx p(\theta|T(y) \in B_\epsilon(D)) = \frac{p(\theta) p(T(y) \in B_\epsilon(D)|\theta)}{p(y=D)}
\end{equation}

For abbrevation we usually denote $p(\theta|T(y) \in B_\epsilon(D))$ as $p_\epsilon(\theta|T(D))$. For simplicity we won't use the function $T$ anymore, but all the expressed ideas are valid even with its existence.


\section{Optimization Monte Carlo (OMC)}

The fundamental idea behind OMC is the creation of a dual problem. We denote as $p_\epsilon(x_1|x_2)$ a kernel distribution; in the simplest case the boxcar kernel. 

\subsection*{Initial View}

\begin{gather} \label{eq:init_view}
  p(y=D|\theta) \approx p(y \in B_\epsilon (D)|\theta) \\
  = p(M_{rand}(\theta) \in B_\epsilon (D)|\theta)\\
  = \int_x p_\epsilon(y=D|x)p(x|\theta)dx \\
  \approx \frac{1}{N} \sum_i^N p_\epsilon (y=D|x=x^{(i)}), x^{(i)} \sim p(x|\theta)
\end{gather}

Inuitively: the probability the random generator to generator artificial data close to $D$ given $\theta$ is \textbf{the summing over all the possible data $x$ the generator can produce given $\theta$ and checking for each one if it is close to $D$}.

\subsection*{Dual View}

\begin{gather} \label{eq:dual_view}
  p(y=D|\theta) \approx p(y \in B_\epsilon (D)|\theta) \\
  = p(M_{rand}(\theta) \in B_\epsilon (D)|\theta)\\
  = \int_x \int_u p_\epsilon(y=D|x)p(x|\theta, u) p(u)dxdu \\
  = \int_u p_\epsilon(y=D|x=M(\theta, u)) p(u)du \\
  \approx \frac{1}{N} \sum_i^N p_\epsilon (y=D|x=M(\theta, u^{(i)}), u^{(i)} \sim p(u)  
\end{gather}

Intuitively: the probability the random generator to generator artificial data close to $D$ given $\theta$ is \textbf{the summing over all the possible nuisance variables and checking for each one if it (deterministically) produces data close to $D$}.


\subsection*{Optimization Problem}

OMC adopts the dual view since it helps continue the Monte Carlo approximation as an optimization problem.

For each sampled set of nuisance variables $u^{(i)}$, we search for a corresponding set $\hat{\theta}^{(i)}$ that maximizes $p_\epsilon (y=D|x=M(\theta, u^{(i)})$ (or in the case of boxcar kernel, it just lies in the permited area).

\begin{subequations}
\begin{alignat}{2}
&\!\max_{\theta}        &\qquad& f(\theta) = p_\epsilon (y=D|x=M(\theta, u^{(i)}))\label{eq:optProb}\\
&\text{subject to} &      & \theta \in \mathbf{R}^D \\
\end{alignat}
\end{subequations}

if $p_\epsilon$ is a Gaussian Kernel, the problem can be transformed to a minimization of a distance:

\begin{subequations}
\begin{alignat}{2}
&\!\min_{\theta}        &\qquad& f(\theta) = \norm{D - M(\theta, u^{(i)}))}^2\label{eq:optProb}\\
&\text{subject to} &      & \theta \in \mathbf{R}^D \\
\end{alignat}
\end{subequations}


(?? Is $M(\theta, u^{(i)})$ a smooth function with respect to $\theta$? If not how is the problem solved ??)


\subsection*{Posterior Representation}

Hypothesis, if for each set of nuisance variables $u^{(i)}$ we have solved the found the $\theta_*^{(i)}$ that produces data as close to $D$, we can represent the posterior as a weighted sum of deltas:


\begin{gather} \label{eq:posterior_as_deltas}
  p(\theta|y=D) \propto p(\theta)p(y=D|\theta) \\
  \propto \frac{1}{N} \sum_i^N w_i p(\theta_*^{(i)})\delta(\theta - \theta_*^{(i))})
\end{gather}


How to compute the weight $w_i$? If $p_\epsilon(y|x)$ is a boxcar kernel, a sensible idea is to count the volume of the area around $\theta_*$ such that we are still inside the limits. If we approximate $M(\theta,u=u^{(i)})$ with a linear plane around $\theta_*^{(i)}$ it can be shown that the volume of the area is given by:


\begin{equation}
  \theta_*^{(i)} = \theta_0^{(i)} + J_0^{-1}(D - M(\theta^{(i)}_0, u^{(i)}))
\end{equation}

\begin{equation}
  V^{(i)} = \frac{\gamma}{\sqrt{det( J_0^{(i)T}J_0^{(i)})}}
\end{equation}

Since $\gamma$ is independent of $(i)$ we can ignore it and set:

\begin{equation}
  w_i = \frac{1}{\sqrt{det( J_0^{(i)T}J_0^{(i)})}}
\end{equation}

Hence, \ref{eq:posterior_as_deltas} can be written as:

\begin{gather} \label{eq:posterior_as_deltas}
  p(\theta|y=D) = \frac{1}{Z} \sum_i^N  \frac{p(\theta_*^{(i)})\delta(\theta - \theta_*^{(i))})}{\sqrt{det( J_0^{(i)T}J_0^{(i)})}}
\end{gather}


\section{Robust Optimization Monte Carlo (ROBC)}

OBC has some disadvantages:

\begin{itemize}
\item For each set of nuisance variables $u^{(i)}$ it relates \textbb{only one} $\theta_*^{(i)}$
\item It shrinks all the area mass (however big this area is) to a single point
  \item The size of the area can be overestimated due to ill conditioned jacobian matrix $J$ and hence dominate the posterior
\end{itemize}



\end{document}