\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{bbold}

\title{Summary of Optimization Monte Carlo methods}
\author{Vasileios Gkolemis}
\date{June 2020}

\newcommand{\norm}[1]{\lVert#1\rVert_2}

\begin{document}

\maketitle

\section{Symbols for functions/probabilities}

nn\begin{itemize}
    \item $p(x|y)$: any valid pdf function $g(x,y): \int_x g(x,y) = 1$. 
    \item $p(x|y=y_0)$ is a function of x (i.e. $g(x): \int_x g(x) = 1$)  
    \item $p(x=x_0|y)$ is a function of y (i.e. $g(y)$) 
    \item $p(x=x_0|y=y_0)$ is a real number $\kappa \in [0,1]$
    \item $p_\epsilon(x)$ is 
\end{itemize}

\section{Symbols for random variables or parameters}

\begin{itemize}
    \item $x, y$ are dummy variables, used as thresholders
    \item $D$ the available data (observed variables)
\end{itemize}



\section{Likelihood-Free methods}

In our models, the random variables are split into 2 categories; the observed ones (visibles) and the unobserved. The unobserved ones can be further split into 3 categories:

\begin{itemize}
    \item the parameters of the model $\theta$, over which we try to infer a posterior distribution
    \item the hidden/latent variables $h$, which we cannot observe but are part of the data generation process
    \item the nuisance variables $u$, which can be though as dummy random variables, that add stochasticity when sampling from a random variable. Simply speaking, those are the r.v. that, when known, can absorb the randomness of the sampling procedure $x \sim p(x|\theta)$, transforming it to a deterministic function $x = f(\theta, u)$
\end{itemize}

The particularity of likelihood-free methods is that the likelihood function $p(y=D|\theta)$ cannot be evaluated (i.e. cannot compute the number $p(y=D|\theta=\theta_0)$) but we are able to sample from the generative distribution $p(y|\theta)$. We can think that we hold a black box simulator (stochastic function) that maps the parameters to a sample $y = M_{rand}(\theta)$. The randomness of the mapping is caused by the unknown nuisance variables $u$ that are part of the generation procedure (e.g. calls to random generator etc). If we represent them with a vector, we can transform the aforementioned random procedure to a deterministic function $M(\theta, u) = y$.  


Likelihood-free methods target on one or more of the following tasks:

\begin{itemize}
  \item to estimate the pdf of the posterior distribution $p(\theta|y=D)$
  \item to get some samples from the posterior distribution $\theta \sim p(\theta|y=D)$
  \item to estimate $E_{P(\theta|y=D)}[h(\theta)]$
 \end{itemize}



\section{Background of Likelihood-Free methods and basic algorithms}

The fundamental idea of all Likelihood-Free methods is that we try to generate (artificial) data using our simulator $M_{rand}(\theta)$ that look-alike (are close) to the observed (real) data $D$. When they do, we hold the parameters $\theta$ that were part of the generation, as possible candidates for the posterior.

In the contiguous case it is impossible to exactly reproduce the data, hence a relaxation is introduced:

\begin{equation} \label{eq:post_approx}
    p(\theta|y=D) \approx p(\theta|y \in B_\epsilon(D)) = \frac{p(\theta) p(y \in B_\epsilon(D)|\theta)}{p(y \in B_\epsilon(D))}
\end{equation}
%
$B_\epsilon(D)$ represents an area (normally sphere or bounding-box) around $D$ governed by the hyper-parameter $\epsilon$. In the limit where $\epsilon \rightarrow 0$, the approximation becomes equality.

When the data belongs to a high-dimensional space (high in our case can be even 10 dimensions), it is becomes difficult to reproduce the observed data $M_{rand}(\theta) \in B_{\epsilon}(D)$ without letting $\epsilon$ reach very big values. Hence, we normally use a way to squeeze our data to a lower dimensions; we use a function $T:\mathbf{R}^{D_1} \rightarrow \mathbf{R}^{D_2}$ where $D_1 > D_2$. Then, equation \ref{eq:post_approx} becomes:

\begin{equation} \label{eq:summary_stat}
    p(\theta|y=D) \approx p(\theta|T(y) \in B_\epsilon(D)) = \frac{p(\theta) p(T(y) \in B_\epsilon(D)|\theta)}{p(y=D)}
\end{equation}

For simplicity we won't use the function $T$ anymore, but all the expressed ideas are valid even with its existence.


\section{Background of Optimization Monte Carlo (OMC) methods}

The fundamental idea behind OMC is the creation of a dual problem. We denote as $p_\epsilon(x_1|x_2)$ a kernel distribution; in the simplest case the boxcar kernel. 

\subsection*{Primal View}

\begin{gather} \label{eq:primal_view}
  L(\theta) = p(y \in B_\epsilon (D)|\theta) = p(M_{rand}(\theta) \in B_\epsilon (D)|\theta)\\
  = \int_x p_\epsilon(y=D|x)p(x|\theta)dx \\
  \approx \frac{1}{N} \sum_i^N p_\epsilon (y=D|x=x^{(i)}), x^{(i)} \sim p(x|\theta)
\end{gather}

Inuitively: the probability the random generator to generator artificial data close to $D$ given $\theta$ is \textbf{the summing over all the possible data $x$ the generator can produce given $\theta$ and checking for each one if it is close to $D$}.

\subsection*{Dual View}

\begin{gather} \label{eq:dual_view}
  L(\theta) = p(y \in B_\epsilon (D)|\theta) = p(M_{rand}(\theta) \in B_\epsilon (D)|\theta)\\
  = \int_x \int_u p_\epsilon(y=D|x)p(x|\theta, u) p(u)dxdu \\
  = \int_u p_\epsilon(y=D|x=M(\theta, u)) p(u)du \\
  \approx \frac{1}{N} \sum_i^N p_\epsilon (y=D|x=M(\theta, u^{(i)}), u^{(i)} \sim p(u)  
\end{gather}

Intuitively: the probability the random generator to generator artificial data close to $D$ given $\theta$ is \textbf{the summing over all the possible nuisance variables and checking for each one if it (deterministically) produces data close to $D$}.



If $p_\epsilon(y=D|x=M(\theta,u^{(i)}))$ is indeed a boxcar kernel, then there is a set $S^{(i)} = \{ \theta: d(D, M(\theta, u^{(i)})) < \epsilon \}$ of $\theta$ values that generate data "close enough" to $D$. Using this kernel we do not express any preference among those $\theta$; we consider them equally probable to have been the parameter setting that generated the data. The probability is inverted proportional to the area the kernel spans and since tha same kernel is used is independent of the sample $u_i$.


\begin{gather} \label{eq:dual_view_analysis}
  L(\theta) = \frac{1}{N} \sum_{i}^Ng_i(\theta) \\
  g_i(\theta) = p_\epsilon(y=D|x=M(\theta, u^{(i}))
= \left\{
	\begin{array}{ll}
		c  & \mbox{if } d(D, M(\theta, u^{(i)})) \leq \epsilon \\
		0 & \mbox{else } 
	\end{array}
  \right. \\
  = c\mathbb{1}_{C_\epsilon^{(i)}}(\theta)
\end{gather}

With these assumptions the posterior distribution can be computed as:

\begin{gather} \label{eq:dual_view_analysis}
  p(\theta|y=D) = \frac{p(\theta)p(y=D|\theta)}{\int_\theta p(\theta)p(y=D|\theta)} \\
  = \frac{1}{Z}p(\theta) \sum_i \mathbb{1}_{C_\epsilon^{(i)}}(\theta) \\
  Z = \int_\theta p(\theta) \sum_i \mathbb{1}_{C_\epsilon^{(i)}}(\theta) d\theta
\end{gather}


Computing the partiotion function and/or sampling from the posterior is not easy, since $\mathbb{1}_{C_\epsilon^{(i)}}(\theta)$ can represent any abstract shape.

Hence, it is imperative to approximate the set $\mathbb{1}_{C_\epsilon^{(i)}}(\theta)$ with a shape that is easier to handle. This is what the OMC and ROMC approaches attempt to do.

\section{OMC algorithm}

OMC approximates each set $\mathbb{1}_{C_\epsilon^{(i)}}(\theta)$ with a weighted delta $w_i\delta(\theta - \theta_*^{(i)})$. Hence the posterior \ref{eq:dual_view_analysis} becomes:

\begin{gather} \label{eq:OMC_posterior}
  p(\theta|y=D) = \frac{\sum_i^N w_i p(\theta_*^{(i)})\delta(\theta - \theta_*^{(i))})}{\sum_i^N w_i p(\theta_*^{(i)})}
\end{gather}

which is very easy to evaluate, sample or compute an expectation on it.

Sampling from \ref{eq:OMC_posterior} is quite easy.

Estimating an expected value can be done through:

\begin{gather} \label{eq:OMC_expectation}
  E_{p(\theta|y=D)}[h(\theta)] = \frac{\sum_i^N w_i p(\theta_*^{(i)})h(\theta_*^{(i)})}{\sum_i^N w_i p(\theta_*^{(i)})}
\end{gather}


The only question is how to compute $\theta_*^{(i)}, w_i$?

Idea: OMC initially approximates $g_i(\theta)$ as an ellipsoid around $\theta_*^{(i)}$ with volume $V^{(i)}$. This ellipsoid is shrinked into a delta placed at its center with weight being its volume. Intuitively, since all values inside the ellipsoid have equal probability of generating the data, their mass is being cummulated in the center point.

Alogorithm for obtaining $\theta_*^{(i)}$, $V^{(i)}$:

Firstly we search for a point inside the boxcar kernel $\theta_0^{(i)}: \mathbb{1}_{C_\epsilon^{(i)}}(\theta_0^{(i)}) = 1$. 

For obtaining such a point, any gradient-based optimiser can be used for solving the following problem:
 
\begin{subequations}
\begin{alignat}{2}
&\!\min_{\theta}        &\qquad& f(\theta) = d(D - M(\theta, u^{(i)}))\label{eq:optProb}\\
&\text{subject to} &      & f(\theta)) < \epsilon \\
\end{alignat}
\end{subequations}

In the simplest case, d can be the euclidean distance.

(Question: Is $M(\theta, u^{(i)})$ a smooth function with respect to $\theta$? If not how is the problem solved ??)

Let's say $\theta_0^{(i)}$ is an approximate solution to the problem i.e. $\theta_0^{(i)} \approx \text{argmin}_\theta f(\theta)$ and $f(\theta_0^{(i)}) < \epsilon$. Then, the central point of the ellipsoid is:

\begin{equation}
  \theta_*^{(i)} = \theta_0^{(i)} + J_0^{-1}(D - M(\theta^{(i)}_0, u^{(i)}))
\end{equation}

and the volume covered is:

\begin{equation}
  V^{(i)} = \frac{\gamma}{\sqrt{det( J_0^{(i)T}J_0^{(i)})}}
\end{equation}

\section{Robust Optimization Monte Carlo (ROBC)}

\subsection{Gradients available}

OBC has some disadvantages:

\begin{itemize}
\item For each set of nuisance variables $u^{(i)}$ it relates only one $\theta_*^{(i)}$ %
\item It shrinks all the area mass (however big this area is) to a single point
  \item The size of the area can be overestimated due to ill conditioned jacobian matrix $J$ and hence dominate the posterior
\end{itemize}

ROBC attempts to solve the two latter problems, by approximating the sets $\mathbb{1}_{C_\epsilon^{(i)}}(\theta)$ as:

$$ \mathbb{1}_{C_\epsilon^{(i)}}(\theta) \approx BB(\theta; \theta_*^{(i)}, k^{(i)}) \cap \mathbb{1}_{C_\epsilon^{(i)}}(\theta)$$

Hence the posterior becomes:

\begin{gather} \label{eq:ROBC_posterior}
  p(\theta|y=D) = \frac{p(\theta)p(y=D|\theta)}{\int_\theta p(\theta)p(y=D|\theta)} \\
  = \frac{1}{Z}p(\theta) \sum_i \mathbb{1}_{C_\epsilon^{(i)}}(\theta)V^{(i)}BB(\theta; \theta_*^{(i)}, k^{(i)}) \\
  Z = \int_\theta p(\theta) \sum_i \mathbb{1}_{C_\epsilon^{(i)}}(\theta)V^{(i)}BB(\theta; \theta_*^{(i)}, k^{(i)}) d\theta
\end{gather}


Computing an expectation $E_{p(\theta|y=D)}[h(\theta)]$ can be done we the following method:

\begin{gather} \label{eq:ROMC_expectation}
  E_{p(\theta|y=D)}[h(\theta)] = \int_\theta h(\theta)p(\theta|y=D)d\theta \\
  = \frac{1}{Z} \int_\theta h(\theta)p(\theta) \sum_i \mathbb{1}_{C_\epsilon^{(i)}}(\theta)V^{(i)}BB(\theta; \theta_*^{(i)}, k^{(i)}) d\theta \\
  = \frac{1}{Z} \sum_i V^{(i)} \int_\theta h(\theta)p(\theta)  \mathbb{1}_{C_\epsilon^{(i)}}(\theta)BB(\theta; \theta_*^{(i)}, k^{(i)}) \frac{q^{(i)}(\theta)}{q^{(i)}(\theta)}d\theta \\  
  = \frac{1}{Z} \sum_i V^{(i)} \sum_j \frac{h(\theta^{(ij)})p(\theta^{(ij)}) \mathbb{1}_{C_\epsilon^{(i)}}(\theta^{(ij)})}{q^{(i)}(\theta^{(ij)})}d\theta \\
  = \frac{1}{Z} \sum_{ij} V^{(i)} \frac{h(\theta^{(ij)})p(\theta^{(ij)}) \mathbb{1}_{C_\epsilon^{(i)}}(\theta^{(ij)})}{q^{(i)}(\theta^{(ij)})}d\theta \\
  = \frac{ \sum_{ij} h(\theta^{(ij)})w_{ij}}{ \sum_{ij}w_{(ij)}}, \\
  w_{ij} = \mathbb{1}_{C_\epsilon^{(i)}}(\theta^{(ij)}) \frac{p(\theta^{(ij)})}{q_{i}(\theta^{(ij)})} \\
\end{gather}


The computation of the bounding box is done, with an algorithm almost like OMC; $\theta_*^{(i)}$ and $J^{(ij)T}J^{(ij)}$ are computed as in OMC and the boundaries are computed as the edge points along the eigenvectors of $J^{(ij)T}J^{(ij)}$.

Hence, ROMC is identical to OMC in the optimization part but differs in the way it represents and samples from the posterior.

\subsection{Gradients not available}

\end{document}