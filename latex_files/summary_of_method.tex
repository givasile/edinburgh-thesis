\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{bbold}

\title{Summary of Optimization Monte Carlo methods}
\author{Vasileios Gkolemis}
\date{June 2020}

\newcommand{\norm}[1]{\lVert#1\rVert_2}

\begin{document}

\maketitle

\section{Symbols for functions/probabilities}

nn\begin{itemize}
    \item $p(x|y)$: any valid pdf function $g(x,y): \int_x g(x,y) = 1$. 
    \item $p(x|y=y_0)$ is a function of x (i.e. $g(x): \int_x g(x) = 1$)  
    \item $p(x=x_0|y)$ is a function of y (i.e. $g(y)$) 
    \item $p(x=x_0|y=y_0)$ is a real number $\kappa \in [0,1]$
    \item $p_\epsilon(x)$ is 
\end{itemize}

\section{Symbols for random variables or parameters}

\begin{itemize}
    \item $x, y$ are dummy variables, used as thresholders
    \item $D$ the available data (observed variables)
\end{itemize}



\section{Likelihood-Free methods}

In our models, the random variables are split into 2 categories; the observed ones (visibles) and the unobserved. The unobserved ones can be further split into 3 categories:

\begin{itemize}
    \item the parameters of the model $\theta$, over which we try to infer a posterior distribution
    \item the hidden/latent variables $h$, which we cannot observe but are part of the data generation process
    \item the nuisance variables $u$, which can be though as dummy random variables, that add stochasticity when sampling from a random variable. Simply speaking, those are the r.v. that, when known, can absorb the randomness of the sampling procedure $x \sim p(x|\theta)$, transforming it to a deterministic function $x = f(\theta, u)$
\end{itemize}

The particularity of likelihood-free methods is that the likelihood function $p(y=D|\theta)$ cannot be evaluated (i.e. cannot compute the number $p(y=D|\theta=\theta_0)$) but we are able to sample from the generative distribution $p(y|\theta)$. We can think that we hold a black box simulator (stochastic function) that maps the parameters to a sample $y = M_{rand}(\theta)$. The randomness of the mapping is caused by the unknown nuisance variables $u$ that are part of the generation procedure (e.g. calls to random generator etc). If we represent them with a vector, we can transform the aforementioned random procedure to a deterministic function $M(\theta, u) = y$.  


Likelihood-free methods target on one or more of the following tasks:

\begin{itemize}
  \item to estimate the pdf of the posterior distribution $p(\theta|y=D)$
  \item to get some samples from the posterior distribution $\theta \sim p(\theta|y=D)$
  \item to estimate $E_{P(\theta|y=D)}[h(\theta)]$
 \end{itemize}



\section{Background of Likelihood-Free methods and basic algorithms}

The fundamental idea of all Likelihood-Free methods is that we try to generate (artificial) data using our simulator $M_{rand}(\theta)$ that look-alike (are close) to the observed (real) data $D$. When they do, we hold the parameters $\theta$ that were part of the generation, as possible candidates for the posterior.

In the contiguous case it is impossible to exactly reproduce the data, hence a relaxation is introduced:

\begin{equation} \label{eq:post_approx}
    p(\theta|y=D) \approx p(\theta|y \in B_\epsilon(D)) = \frac{p(\theta) p(y \in B_\epsilon(D)|\theta)}{p(y \in B_\epsilon(D))}
\end{equation}
%
$B_\epsilon(D)$ represents an area (normally sphere or bounding-box) around $D$ governed by the hyper-parameter $\epsilon$. In the limit where $\epsilon \rightarrow 0$, the approximation becomes equality.

When the data belongs to a high-dimensional space (high in our case can be even 10 dimensions), it is becomes difficult to reproduce the observed data $M_{rand}(\theta) \in B_{\epsilon}(D)$ without letting $\epsilon$ reach very big values. Hence, we normally use a way to squeeze our data to a lower dimensions; we use a function $T:\mathbf{R}^{D_1} \rightarrow \mathbf{R}^{D_2}$ where $D_1 > D_2$. Then, equation \ref{eq:post_approx} becomes:

\begin{equation} \label{eq:summary_stat}
    p(\theta|y=D) \approx p(\theta|T(y) \in B_\epsilon(D)) = \frac{p(\theta) p(T(y) \in B_\epsilon(D)|\theta)}{p(y=D)}
\end{equation}

For simplicity we won't use the function $T$ anymore, but all the expressed ideas are valid even with its existence.


\section{Background of Optimization Monte Carlo (OMC) methods}

The fundamental idea behind OMC is the creation of a dual problem. We denote as $p_\epsilon(x_1|x_2)$ a kernel distribution; in the simplest case the boxcar kernel. 

\subsection*{Primal View}

\begin{gather} \label{eq:primal_view}
  L(\theta) = p(y \in B_\epsilon (D)|\theta) = p(M_{rand}(\theta) \in B_\epsilon (D)|\theta)\\
  = \int_x p_\epsilon(y=D|x)p(x|\theta)dx \\
  \approx \frac{1}{N} \sum_i^N p_\epsilon (y=D|x=x^{(i)}), x^{(i)} \sim p(x|\theta)
\end{gather}

Inuitively: the probability the random generator to generator artificial data close to $D$ given $\theta$ is \textbf{the summing over all the possible data $x$ the generator can produce given $\theta$ and checking for each one if it is close to $D$}.

\subsection*{Dual View}

\begin{gather} \label{eq:dual_view}
  L(\theta) = p(y \in B_\epsilon (D)|\theta) = p(M_{rand}(\theta) \in B_\epsilon (D)|\theta)\\
  = \int_x \int_u p_\epsilon(y=D|x)p(x|\theta, u) p(u)dxdu \\
  = \int_u p_\epsilon(y=D|x=M(\theta, u)) p(u)du \\
  \approx \frac{1}{N} \sum_i^N p_\epsilon (y=D|x=M(\theta, u^{(i)}), u^{(i)} \sim p(u)  
\end{gather}

Intuitively: the probability the random generator to generator artificial data close to $D$ given $\theta$ is \textbf{the summing over all the possible nuisance variables and checking for each one if it (deterministically) produces data close to $D$}.


\begin{gather}
  L(\theta) = \frac{1}{N} \sum_{i}^Ng_i(\theta) \\
  g_i(\theta) = p_\epsilon(y=D|x=M(\theta, u^{(i})) = \\ 
= \left\{
	\begin{array}{ll}
		c_i  & \mbox{if } \norm{D-M(\theta, u^{(i)})} \leq \epsilon \\
		0 & \mbox{else } 
	\end{array}
  \right. \\
  = c_i\mathbb{1}_{C_\epsilon^{(i)}}(\theta)
  \end{gather}


\section{OMC algorithm}

Idea: OMC algorithm approximates $g_i(\theta)$ as an ellipsoid around $\theta_0^{(i)}$, which is any value inside the boxcar kernel $\theta_0^{(i)}: \mathbb{1}_{C_\epsilon^{(i)}}(\theta) = 1$. For obtaining such a point, any gradient-based optimiser can be used for solving the following problem:
 
\begin{subequations}
\begin{alignat}{2}
&\!\min_{\theta}        &\qquad& f(\theta) = \norm{D - M(\theta, u^{(i)}))}^2\label{eq:optProb}\\
&\text{subject to} &      & \theta \in \mathbf{R}^D \\
&                  &      & \norm{D - M(\theta, u^{(i)}))} < \epsilon \\
\end{alignat}
\end{subequations}
%
(Question: Is $M(\theta, u^{(i)})$ a smooth function with respect to $\theta$? If not how is the problem solved ??)

The central point of the ellipsoid is:

\begin{equation}
  \theta_*^{(i)} = \theta_0^{(i)} + J_0^{-1}(D - M(\theta^{(i)}_0, u^{(i)}))
\end{equation}

and the volume covered is:

\begin{equation}
  V^{(i)} = \frac{\gamma}{\sqrt{det( J_0^{(i)T}J_0^{(i)})}}
\end{equation}

If we represent shrink all its mass in the center and use its volume as an indicator of its weight:

\begin{gather} \label{eq:OMC_posterior}
  p(\theta|y=D) = \frac{p(\theta)p(y=D|\theta)}{Z} \\
  = \frac{\sum_i^N w_i p(\theta_*^{(i)})\delta(\theta - \theta_*^{(i))})}{\sum_i^N w_i p(\theta_*^{(i)})}
\end{gather}

\begin{equation}
  w_i = \frac{1}{\sqrt{det( J_0^{(i)T}J_0^{(i)})}}
\end{equation}


Sampling from \ref{eq:OMC_posterior} is quite easy.

Estimating an expected value can be done through:

\begin{gather} \label{eq:OMC_expectation}
  E_{p(\theta|y=D)}[h(\theta)] = \frac{\sum_i^N w_i p(\theta_*^{(i)})h(\theta_*^{(i)})}{\sum_i^N w_i p(\theta_*^{(i)})}
\end{gather}


\section{Robust Optimization Monte Carlo (ROBC)}

\subsection{Gradients available}

OBC has some disadvantages:

\begin{itemize}
\item For each set of nuisance variables $u^{(i)}$ it relates only one $\theta_*^{(i)}$ %
\item It shrinks all the area mass (however big this area is) to a single point
  \item The size of the area can be overestimated due to ill conditioned jacobian matrix $J$ and hence dominate the posterior
\end{itemize}


\subsection{Gradients not available}

\end{document}