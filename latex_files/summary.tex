\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bbold}

\title{Summary of Optimization Monte Carlo methods}
\author{Vasileios Gkolemis}
\date{June 2020}

\newcommand{\norm}[1]{\lVert#1\rVert_2}

\begin{document}

\maketitle

\section{Symbols for functions/probabilities}\label{sec:symbols-functions}

\begin{itemize}
    \item $p(x|y)$: any valid pdf function $g(x,y): \int_x g(x,y) = 1$. 
    \item $p(x|y=y_0)$ is a function of x (i.e. $g(x): \int_x g(x) = 1$)  
    \item $p(x=x_0|y)$ is a function of y (i.e. $g(y)$) 
    \item $p(x=x_0|y=y_0)$ is a real number $\kappa \in [0,1]$
\end{itemize}

\section{Symbols for random variables or parameters} \label{sec:symbols-random_variables}

\begin{itemize}
    \item $x, y$ are dummy variables, used as placeholders
    \item $D$ the available data (observed variables)
\end{itemize}


\section{Likelihood-Free methods Introduction}\label{sec:likelihood-free-methods-introduction}

In probabilistic models, the random variables are split into 2 categories;
the observed ones (visibles) and the unobserved.
The unobserved variables can be furtherly split into 3 categories:

\begin{itemize}
    \item the parameters of the model $\theta$, over which we try to infer a posterior distribution
    \item the hidden/latent variables $h$, which we cannot observe but are part of the data generation process
    \item the nuisance variables $u$, which can be though as dummy random variables that model the randomness when sampling from a distribution.
    Hence, when the value of the nuisance variables is known, the sampling procedure $x \sim p(x|\theta)$ transforms to a deterministic function $x = f(\theta,u)$
\end{itemize}

The particularity of likelihood-free methods is that we cannot evaluate the likelihood function $p(y|\theta)$ but we are able to sample from it.
Intuitively, we can think of holding a black box simulator (stochastic function) that given a set of parameters,
can generate samples $y \sim M_{rand}(\theta)$.
As explained above, the random generator can be transformed to a deterministic, by modelling the nuisance variables;
hence $y=M(\theta, u)$.


Likelihood-free methods target the following tasks:

\begin{itemize}
  \item to estimate the pdf of the posterior distribution $p(\theta|y=D)$
  \item to get some samples from the posterior distribution $\theta \sim p(\theta|y=D)$
  \item to estimate $E_{P(\theta|y=D)}[h(\theta)]$
 \end{itemize}



\section{Background of Likelihood-Free methods}\label{sec:background-of-likelihood-free-methods}

The fundamental idea of all Likelihood-Free methods is that we try to generate (artificial) data,
using our simulator $M_{rand}(\theta)$ that is close to the observed (real) data $D$.
If we achieve so, the parameters $\theta$that produced those data are "kept" as candidates of the posterior.

In the contiguous case it is impossible to exactly reproduce the data, hence a relaxation is introduced:

\begin{equation} \label{eq:post_approx}
    p(\theta|y=D) \approx p(\theta|y \in B_\epsilon(D)) = \frac{p(\theta) p(y \in B_\epsilon(D)|\theta)}{p(y \in B_\epsilon(D))}
\end{equation}
%
$B_\epsilon(D)$ represents an area around $D$ governed by the hyper-parameter $\epsilon$.
In the limit where $\epsilon \rightarrow 0$, the approximation becomes equality.

When the data belongs to a high-dimensional space (high in our methods can be even 10 dimensions),
it becomes difficult to reproduce the observed data $M_{rand}(\theta) \in B_{\epsilon}(D)$ without increasing
$\epsilon$ exponentially (curse of dimensionality).
Hence, we normally use a way to project our data to a lower dimension;
we use a function $T:\mathbf{R}^{D_1} \rightarrow \mathbf{R}^{D_2}$ where $D_1 > D_2$.
Then, the equation \ref{eq:post_approx} becomes:

\begin{equation} \label{eq:summary_stat}
    p(\theta|y=D) \approx p(\theta|T(y) \in B_\epsilon(D)) = \frac{p(\theta) p(T(y) \in B_\epsilon(D)|\theta)}{p(y=D)}
\end{equation}

For simplicity we won't use the function $T$ anymore, but all the expressed ideas can be easily applied in the presence of $T$.


\section{Background of Optimization Monte Carlo (OMC) methods}\label{sec:background-of-omc}

The fundamental idea behind OMC is the creation of a dual problem.
We denote as $p_\epsilon(x_1|x_2)$ a kernel distribution; in the simplest case the boxcar kernel.

\subsection*{Primal View}

\begin{gather} \label{eq:primal_view}
  L(\theta) = p(y \in B_\epsilon (D)|\theta) = p(M_{rand}(\theta) \in B_\epsilon (D)|\theta)\\
  = \int_x p_\epsilon(y=D|x)p(x|\theta)dx \\
  \approx \frac{1}{N} \sum_i^N p_\epsilon (y=D|x=x^{(i)}), x^{(i)} \sim p(x|\theta)
\end{gather}
%
Inuitively: the probability the parameters $\theta$ to have generated the data $D$ is \textbf{the summing over all possible data $x$ the simulator can produce and checking for each one if it is close to $D$}.

\subsection*{Dual View}

\begin{gather} \label{eq:dual_view}
  L(\theta) = p(y \in B_\epsilon (D)|\theta) = p(M_{rand}(\theta) \in B_\epsilon (D)|\theta)\\
  = \int_x \int_u p_\epsilon(y=D|x)p(x|\theta, u) p(u)dxdu \\
  = \int_u p_\epsilon(y=D|x=M(\theta, u)) p(u)du \\
  \approx \frac{1}{N} \sum_i^N p_\epsilon (y=D|x=M(\theta, u^{(i)}), u^{(i)} \sim p(u)
\end{gather}

Intuitively: the probability the parameters $\theta$ to have generated the data $D$ is \textbf{the summing over all the possible nuisance variables and checking for each one if it (deterministically) produces data close to $D$}.


\subsection*{Setting $p_{\epsilon}(y|x)$ to be the boxcar kernel}

If $p_\epsilon(y=D|x=M(\theta,u^{(i)}))$ is a boxcar kernel:

\begin{gather}
  p_\epsilon(y=D|x=M(\theta,u^{(i)})) = \left\{
	\begin{array}{ll}
		c  & \mbox{if } d(D, M(\theta, u^{(i)})) \leq \epsilon \\
		0 & \mbox{else } 
	\end{array}
  \right. \\
\end{gather}
%
then there is a set $S^{(i)} = \{ \theta: d(D, M(\theta, u^{(i)})) < \epsilon \}$ of $\theta$ values that generate data "close enough" to $D$. Using this kernel we do not express any preference among those $\theta$; we consider them equally probable to have been the parameter setting that generated the data, with probability $c$. The size of the set $S^{(i)}$ is related to $\epsilon$ in a positive way; increasing $\epsilon$ increases $S^{(i)}$ but in a non-linear way. We have to investigate more this relationship.

The value of $c$ is the volume of the accepatnce area and is related to (a) the dimensionality of the problem $d$ (b) the radius $\epsilon$ and (c) the type of the distance $d(\cdot, \cdot)$. Hence, it is independendent of the size of the set $S^{(i)}$ and is constant for all nuisance variables $u_i$ (i.e. independent of $(i)$). If the distance is defined as the euclidean distance $d(\cdot, \cdot) = \norm{\cdot, \cdot}$, then $c= \frac{1}{V}$ where $V$ is the volume of the $n-ball$:

  \begin{gather}
    V = \frac{\pi^{\frac{d}{2}}}{\Gamma(\frac{d}{2} +1)}\epsilon^d \\
    \Gamma(d) = (d-1)! \\
    \Gamma(d + \frac{1}{2}) = (d-\frac{1}{2})(d-\frac{3}{2})\cdot \cdot \frac{1}{2}\pi^{\frac{1}{2}}
  \end{gather}

 We observe that $c \propto \frac{1}{\epsilon^d}$. Hence, in high-dimensionality (i.e. $d \geq 10$), if $\epsilon > 1$, $c$ becomes very small. On the other hand, if $\epsilon < 1$, $c$ becomes very large.


 Intuition: if $c$ is large, if a specific $\theta$ is inside the boxcar kernel it has large probability to be the $\theta$ that generated the data. If $c$ is small, $\theta$ inside the boxcar kernel has small probability to have generated the data. 

 \subsection*{Posterior when $p_{\epsilon}(y|x)$ is the boxcar kernel}
 
\begin{gather} \label{eq:dual_view_analysis_1}
  L(\theta) = \frac{1}{N} \sum_{i}^N g_i(\theta) \\
  g_i(\theta) = p_\epsilon(y=D|x=M(\theta, u^{(i}))
= \left\{
	\begin{array}{ll}
		c  & \mbox{if } d(D, M(\theta, u^{(i)})) \leq \epsilon \\
		0 & \mbox{else } 
	\end{array}
  \right. \\
  = c\mathbb{1}_{C_\epsilon^{(i)}}(\theta)
\end{gather}

With these assumptions the posterior distribution can be computed as:

\begin{gather} \label{eq:dual_view_analysis_2}
  p(\theta|y=D) = \frac{p(\theta)p(y=D|\theta)}{\int_\theta p(\theta)p(y=D|\theta)} \\
  = \frac{1}{Z}p(\theta) \sum_i \mathbb{1}_{C_\epsilon^{(i)}}(\theta) \\
  Z = \int_\theta p(\theta) \sum_i \mathbb{1}_{C_\epsilon^{(i)}}(\theta) d\theta
\end{gather}

We observe that $c$ disappears. There are 2 difficulties with using this posterior:
\begin{itemize}
\item Computing the partition function is difficult since $\mathbb{1}_{C_\epsilon^{(i)}}(\theta)$ can represent any abstract shape
  \item Evaluating a specific $\theta=\theta_0$ requires $N$ forward-passes of the simulator (one for each nuisance variable) which can be expensive.
  \end{itemize}


\subsection*{Expectation on posterior, when $p_{\epsilon}(y|x)$ is the boxcar kernel}

\begin{gather} \label{eq:exp_post_1}
  E_{p(\theta|y=D)}[h(\theta)] = \int_\theta h(\theta)p(\theta|y=D)d\theta \\
  = \frac{1}{Z} \int_\theta h(\theta)p(\theta) \sum_i \mathbb{1}_{C_\epsilon^{(i)}}(\theta) d\theta \\
  Z = \int_\theta p(\theta) \sum_i \mathbb{1}_{C_\epsilon^{(i)}}(\theta) d\theta
\end{gather}

Changing the order:

\begin{gather} \label{eq:exp_post_2}
  E_{p(\theta|y=D)}[h(\theta)] = \frac{1}{Z} \sum_i \int_\theta h(\theta)p(\theta) \mathbb{1}_{C_\epsilon^{(i)}}(\theta) d\theta \\
  Z = \sum_i \int_\theta p(\theta) \mathbb{1}_{C_\epsilon^{(i)}}(\theta) d\theta
\end{gather}


Both integrals can be approximated by inventing a proposal distribution $q^{(i)}$ for each nuisance variable:

\begin{gather} \label{eq:exp_post_appr}
  E_{p(\theta|y=D)}[h(\theta)] \approx \frac{1}{Z} \sum_i \sum_j h(\theta^{(ij)})\frac{p(\theta^{(ij)})}{q(\theta^{(ij)})} \mathbb{1}_{C_\epsilon^{(i)}}(\theta^{(ij)}) \\
  Z = \sum_i \sum_j \frac{p(\theta^{(ij)})}{q(\theta^{(ij)})} \mathbb{1}_{C_\epsilon^{(i)}}(\theta^{(ij)}) d\theta \\
  \text{where  } \theta^{(ij)} \sim q^{(i)}
\end{gather}

  
Here, the difficult part is to define proposal regions $q^{(i)}$ with high acceptance rate i.e. $r = \frac{\sum_i \sum_j \mathbb{1}_{C_\epsilon^{(i)}}(\theta^{(ij)})}{NM}$ to be as high as possible. As a second goal, the fractions $\frac{p(\theta^{(ij)})}{q(\theta^{(ij)})}$ should be as high as possible.


\section{OMC algorithm}

OMC approximates each set $\mathbb{1}_{C_\epsilon^{(i)}}(\theta)$ with a weighted delta $w_i\delta(\theta - \theta_*^{(i)})$. Hence the posterior \ref{eq:dual_view_analysis} becomes:

\begin{gather} \label{eq:OMC_posterior}
  p(\theta|y=D) = \frac{\sum_i^N w_i p(\theta_*^{(i)})\delta(\theta - \theta_*^{(i))})}{\sum_i^N w_i p(\theta_*^{(i)})}
\end{gather}
%
which is very easy to evaluate, sample or compute an expectation on it.
Estimating an expected value can be done through:

\begin{gather} \label{eq:OMC_expectation}
  E_{p(\theta|y=D)}[h(\theta)] = \frac{\sum_i^N w_i p(\theta_*^{(i)})h(\theta_*^{(i)})}{\sum_i^N w_i p(\theta_*^{(i)})}
\end{gather}


The only question is how to compute $\theta_*^{(i)}, w_i$; OMC initially approximates $g_i(\theta)$ as an ellipsoid around $\theta_*^{(i)}$ with volume $V^{(i)}$. This ellipsoid is then shrinked into a delta placed at its center with weight being its volume. Intuitively, since all values inside the ellipsoid have equal probability of generating the data, their mass is being cummulated in the center point.

\subsubsection*{Alogorithm for obtaining $\theta_*^{(i)}$, $V^{(i)}$}

Firstly we search for a point inside the boxcar kernel $\theta_0^{(i)}: \mathbb{1}_{C_\epsilon^{(i)}}(\theta_0^{(i)}) = 1$. 
%
For obtaining such a point, any gradient-based optimiser can be used for solving the following problem:
 
\begin{subequations}
\begin{alignat}{2}
&\!\min_{\theta}        &\qquad& f(\theta) = d(D - M(\theta, u^{(i)}))\label{eq:optProb}\\
&\text{subject to} &      & f(\theta)) < \epsilon \\
\end{alignat}
\end{subequations}

In the simplest case, d can be the euclidean distance. Let's say $\theta_0^{(i)}$ is an approximate solution to the problem i.e. $\theta_0^{(i)} \approx \text{argmin}_\theta f(\theta)$ and $f(\theta_0^{(i)}) < \epsilon$, as returned by the optimizer. Then, the central point of the ellipsoid is:

\begin{equation}
  \theta_*^{(i)} = \theta_0^{(i)} + J_0^{-1}(D - M(\theta^{(i)}_0, u^{(i)}))
\end{equation}
%
and the volume covered is:
%
\begin{equation}
  V^{(i)} = \frac{\gamma}{\sqrt{det( J_0^{(i)T}J_0^{(i)})}}
\end{equation}
%
(Question: Is $M(\theta, u^{(i)})$ a smooth function with respect to $\theta$? If not how is the problem solved?)

\section{Robust Optimization Monte Carlo (ROBC)}

OBC has some disadvantages:

\begin{itemize}
\item For each set of nuisance variables $u^{(i)}$ it relates only one $\theta_*^{(i)}$ i.e. single-mode function
\item It shrinks all the area mass (however big this area is) to a single point
  \item The size of the area can be overestimated due to ill conditioned jacobian matrix $J$ and hence dominate the posterior
\end{itemize}

\subsection{Gradients available}


ROBC attempts to solve the two latter problems, by approximating the sets $\mathbb{1}_{C_\epsilon^{(i)}}(\theta)$ as:

$$ \mathbb{1}_{C_\epsilon^{(i)}}(\theta) \approx \mathbb{1}_{C_{\text{BB}}^{(i)}}(\theta) \cap \mathbb{1}_{C_\epsilon^{(i)}}(\theta)$$
%
Intuitively: We design one bounding box around a \textbf{single} continuous set of $\mathbb{1}_{C_\epsilon^{(i)}}(\theta)$. If $\mathbb{1}_{C_\epsilon^{(i)}}(\theta)$ includes multiple disjoint sets, we extract only one of them. The bounding box must be big enough to include all the specific set of $\mathbb{1}_{C_\epsilon^{(i)}}(\theta)$ (for not deleting parts of the posterior) and also small enough since it will serve as a proposal region for drawing samples.

Hence the posterior becomes:

\begin{gather} \label{eq:ROBC_posterior}
  p(\theta|y=D) = \frac{p(\theta)p(y=D|\theta)}{\int_\theta p(\theta)p(y=D|\theta)} \\
  = \frac{1}{Z}p(\theta) \sum_i \mathbb{1}_{C_\epsilon^{(i)}}(\theta)\mathbb{1}_{C_{\text{BB}}^{(i)}}(\theta) \\
  Z = \int_\theta p(\theta) \sum_i \mathbb{1}_{C_\epsilon^{(i)}}(\theta)\mathbb{1}_{C_{\text{BB}}^{(i)}}(\theta) d\theta
\end{gather}


Computing an expectation $E_{p(\theta|y=D)}[h(\theta)]$ can be done from \ref{eq:exp_post_appr} with just setting  $q^{(i)}$ to be a uniform distribution over the bounding box.


\subsubsection*{Algorithm for obtaining $\theta_*^{(i)}$, $V^{(i)}$}

The task is computing the $BB^{(i)}$. The central point $\theta_*^{(i)}$ and the jacobian at this point $J^{(ij)T}J^{(ij)}$ can be computed exactly as in OMC. The boundaries are computed as the edge points along the eigenvectors of $J^{(ij)T}J^{(ij)}$.

Hence, ROMC is identical to OMC in the optimization part but differs in the way it represents and samples from the posterior.

\subsection{Gradients not available}



\subsection*{Questions}

Some open questions to ask to Michael at the call

\begin{itemize}
\item Is $M(\theta, u)$ smooth w.r.t. $\theta$? what is exactly the nuisance variables? If it is the seed it can take only integer values? Yes, we can suppose that $M(\theta, u)$ smooth w.r.t. $\theta$. In general, the nuisance variables are the underlying random numbers that are generated under the hood when a random generator is called. At high level when calling a random routine, the nuisance variables could be the random seed.
\item How ROMC solves the problem of ill-conditioned Jacobian matrix? In option 1, even though the proposal region is overestimated, the samples are filtered by $\mathbb{1}_{C_\epsilon^{(i)}}(\theta)$ so the samples are not going to be overestimatedProbably refers to the GP option.
\item In OMC the central point of the ellipsoid is not the outcome of the optimizer (distinction between $\theta_0$ and $\theta_*$)
\item Can we sample from the posterior of $ROMC$?
\item Questions about the case that we do not have gradients. After we get the boundary box with the algorithm, why we fit a regression model? Why we don't keep the box? 
  \item How to start coding at ELFI?
  \end{itemize}


  Some ideas

  \begin{itemize}
  \item The ROMC method has something that is not very intuitive to me. It defines a bounding box as the proposal area and then it accepts the samples if they are inside the acceptance region; it doesn't attach any weight to the generated samples. This means for example that if there is a very small set of acceptable $\theta$ and we draw a very small rectangle around it as the proposal region, almost all generated samples are accepted, this very small region will be overly-represented with too many samples in the posterior. Shouldn't we make a normalisation?

    \textbf{Normalisation Idea}

    if $V^{i}$ is the volume of the approximate (loose) bounding box, $N$ the number of samples generated from itand $N_1^{(i)}$ the number of accepted samples, then approximate volume of the acceptance area is $\hat{V}^{(i)} = ||\mathbb{1}_{C_\epsilon^{(i)}}(\theta)|| \approx \frac{N_1^{(i)}}{N}V^{(i)}$. Hence, in order to keep the proportionalities of the regions and not overly-represent some areas the following should hold. If $\bar{\theta}^{(i)}=[\theta^{(i)}_1, ..., \theta^{(i)}_{N_1^{(i)}}]$, $N_1^{(i)}$ the current number of elements of the set and the $\hat{N}_1^{(i)}$ the final number of elements after interpolating/discarding, then

    $$\frac{\hat{V}^{(i)}}{\hat{V}^{(j)}} \approx \frac{N_1^{(i)}}{N_1^{(j)}}\frac{V^{(i)}}{V^{(j)}} \approx \frac{\hat{N}_1^{(i)}}{\hat{N}_1^{(j)}}$$

    So a simple method of interpolating points must be implemented.
    
  \item If instead of boxcar kernel we use a Gaussian $p(y=D|x) = \mathbb{N}(y; x, \sigma^2)$ then the optimizer will return $\theta_0$; then $g_i(\theta)$ becomes a Gaussian as well? If this is the case, then the posterior becomes a mixture of Gaussians which seems convenient.
\end{itemize}

\section{Implementation}

Symbols:
\begin{itemize}
  \item dim: The dimensionality of the inference problem. On how many parameters I want to define the posterior.
  \end{itemize}

\subsection{Parameter Inference}

\subsection{Utils}

\subsubsection{Classes}

\begin{itemize}
\item $NDimBoundingBox$
\item $OptimisationProblem$
  \item $RomcPosterior$
  \end{itemize}


\subsubsection{Functions}

Three functions for creating the Bounding Box region:

\begin{itemize}
  \item gt\_around\_theta: ($\theta_0$, func, lim, step, eps) $\rightarrow$ List[nDimBoundingBox]: It measures the bounding box by following the canonical directions and moving one step at a time, until $f(\theta_0 + ke_d) > eps$. So $k=i*step$. Works for all dimensionalities. Complexity: $\mathcal{O}(dim \times step \times lim)$
  
  \item romc\_jacobian: ($\theta_0$, func, lim, step, eps) $\rightarrow$ List[nDimBoundingBox]: It measures the bounding box by following the eigenvectors of the curvature at the point $\theta_0$ and moving one step at a time, until $f(\theta_0 + ke_d) > eps$. So $k=i*step$. Works for all dimensionalities. Complexity: $\mathcal{O}(dim \times step \times lim)$

  \item gt\_full\_coverage:($\theta_0$, func, left\_lim, right\_lim, step, eps) $\rightarrow$ List[nDimBoundingBox]: It computes \textbf{all} bounding boxes inside the square region defined by left\_lim, right\_lim parameters. In all dimensions it follows the canonical directions. Works only for 1D case. Complexity: $\mathcal{O}(dim=1 \times right\_lim \times left\_lim \times step)$
  \end{itemize}

  Create deterministic functions utility:

  \begin{itemize}
    \item create\_deterministic\_generator: wrapper functions that gets the ElfiModel and the nuisance variable and returnes a function: (theta\_flat) $\rightarrow$ (dictionary with all output nodes)
    \item create\_output\_function: selcts only the output node from the deterministic generator      \end{itemize}
  

  Utility functions:

  \begin{itemize}
    \item collect\_solutions (OptimisationProblems) $\rightarrow$ (bounding\_boxes, funcs, funcs\_unique, nuisance)): Collects all the solutions already obtained (i.e. all the solved optimisation problems for which at least one bounding box region is already obtained). Used before defining the ROMC posterior.
    \end{itemize}

\end{document}